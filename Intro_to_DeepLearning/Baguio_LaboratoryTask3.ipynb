{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f727b91-e695-42bb-ac1e-13aba73fe450",
   "metadata": {},
   "source": [
    "<center>Elective 4</center>\n",
    "<center>Deep Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58632c44-759c-4fb1-a1f7-9011f7341097",
   "metadata": {},
   "source": [
    "## **Advisor:** Jhun Brian Andam\n",
    "## **Student:** Christian Jay Baguio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e247b-9fef-436f-8645-5d86af4ebbc1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"font-family: Times New Roman\">\n",
    "    <h4><strong>Laboratory Task 3</strong></h4>\n",
    "\n",
    "<p style=\"font-family:Times New Roman; text-align:justify; font-size:15px\">\n",
    "    <b>Instruction:</b> Perform a forward and backward propagation in python using the inputs from <b>Laboratory Task 2</b>\n",
    "</p>\n",
    "\n",
    "```python\n",
    "x = np.array([1, 0, 1])\n",
    "y = np.array([1])\n",
    "\n",
    "# use relu as the activation function.\n",
    "\n",
    "# learning rate\n",
    "lr = 0.001\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8011fb9c-9429-4e92-a69d-1a4da3dc3fbd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74937b4-faf3-4a16-bb47-1fce6079ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04620a6b-cba0-49a7-bdcf-aef9ec9eb315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: [0.4232]\n",
      "Updated weights and biases:\n",
      "W_hidden: [[ 0.2      -0.300184]\n",
      " [ 0.4       0.1     ]\n",
      " [-0.5       0.199816]]\n",
      "b_hidden: [-0.4       0.199816]\n",
      "W_output: [-0.3      -0.199908]\n",
      "b_output: [0.10092]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 0, 1])\n",
    "y = np.array([1])\n",
    "\n",
    "#Weights para sa hidden layer gikan sa task 2\n",
    "W_hidden = np.array([[0.2, -0.3], \n",
    "                     [0.4, 0.1], \n",
    "                     [-0.5, 0.2]])\n",
    "\n",
    "#Biases gikan sa hidden layer\n",
    "b_hidden = np.array([-0.4, 0.2])\n",
    "\n",
    "#Weights para sa output layer gikan task 2\n",
    "W_output = np.array([-0.3, -0.2])\n",
    "\n",
    "#Bias for output layer\n",
    "b_output = 0.1\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#Activation function: Rectified Linear Unit or inshort ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "#Derivative sa ReLU para backpropagation\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(x):\n",
    "    Z_hidden = np.dot(x, W_hidden) + b_hidden  #Hidden layer calc\n",
    "    A_hidden = relu(Z_hidden)  #Apply ang ReLU activation\n",
    "    \n",
    "    Z_output = np.dot(A_hidden, W_output) + b_output #Output layer calc\n",
    "    A_output = Z_output  #Linear activation for output (walay ReLU na apply diri)\n",
    "    \n",
    "    return Z_hidden, A_hidden, Z_output, A_output\n",
    "\n",
    "#MSE loss function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return 0.5 * np.square(y_true - y_pred)\n",
    "\n",
    "#Backward propagation\n",
    "def backward_propagation(x, y, Z_hidden, A_hidden, A_output):\n",
    "    #Compute gradient of the loss w.r.t. output layer (dL/dA_output)\n",
    "    dL_dA_output = A_output - y\n",
    "    \n",
    "    #Gradients para output layer weights and bias\n",
    "    dL_dW_output = dL_dA_output * A_hidden\n",
    "    dL_db_output = dL_dA_output\n",
    "    \n",
    "    #Gradients para hidden layer (apply ang chain rule)\n",
    "    dA_hidden_dZ_hidden = relu_derivative(Z_hidden)\n",
    "    dL_dZ_hidden = dL_dA_output * W_output * dA_hidden_dZ_hidden\n",
    "    \n",
    "    dL_dW_hidden = np.outer(x, dL_dZ_hidden)  #Gradients for W_hidden\n",
    "    dL_db_hidden = dL_dZ_hidden #Gradients for biases\n",
    "    \n",
    "    return dL_dW_output, dL_db_output, dL_dW_hidden, dL_db_hidden\n",
    "\n",
    "#Perform forward pass\n",
    "Z_hidden, A_hidden, Z_output, A_output = forward_propagation(x)\n",
    "\n",
    "#Calc loss\n",
    "loss = mse_loss(y, A_output)\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "#Perform backward pass\n",
    "dL_dW_output, dL_db_output, dL_dW_hidden, dL_db_hidden = backward_propagation(x, y, Z_hidden, A_hidden, A_output)\n",
    "\n",
    "#Update weights and biases using gradient descent\n",
    "W_output -= lr * dL_dW_output\n",
    "b_output -= lr * dL_db_output\n",
    "W_hidden -= lr * dL_dW_hidden\n",
    "b_hidden -= lr * dL_db_hidden\n",
    "\n",
    "print(\"Updated weights and biases:\")\n",
    "print(\"W_hidden:\", W_hidden)\n",
    "print(\"b_hidden:\", b_hidden)\n",
    "print(\"W_output:\", W_output)\n",
    "print(\"b_output:\", b_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb3b53-7a40-4802-b928-7886a236bbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
